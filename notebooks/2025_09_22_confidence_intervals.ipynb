{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1907513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8921edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.825\n",
      "Std: 0.035\n",
      "Std Error: 0.025\n",
      "CI: (0.507, 1.143)\n"
     ]
    }
   ],
   "source": [
    "# scores = np.array([0.8, 0.85, 0.82, 0.8, 0.83])\n",
    "scores = np.array([0.8, 0.85])\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std(ddof=1)\n",
    "score_se = score_std / np.sqrt(len(scores))\n",
    "score_ci = t.interval(0.95, len(scores) - 1, loc=score_mean, scale=score_se)\n",
    "print(f\"Mean: {score_mean:.3f}\")\n",
    "print(f\"Std: {score_std:.3f}\")\n",
    "print(f\"Std Error: {score_se:.3f}\")\n",
    "print(f\"CI: ({score_ci[0]:.3f}, {score_ci[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7db1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7896446609406726, 0.8603553390593273)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_mean - score_std, score_mean + score_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7762360b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731d2792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182.0, 182.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.8, 0.85])\n",
    "b = np.array([100, 120])\n",
    "a @ b, 0.8 * 100 + 0.85 * 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43095fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7297734627831716, (0.6934218906320906, 0.7632861592599165)),\n",
       " 0.7297734627831716)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "# --- Primary: Wilson CI on micro accuracy ---\n",
    "def wilson_ci(num_correct, num_samples, alpha=0.05):\n",
    "    \"\"\"Wilson CI on micro accuracy\n",
    "    \n",
    "    Args:\n",
    "        num_correct: number correct from ONE out-of-fold prediction per sample (possibly after averaging probs across repeats)\n",
    "        num_samples: total samples\n",
    "        alpha: significance level (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "        p: proportion correct\n",
    "        ci: tuple of lower and upper bounds of the Wilson CI\n",
    "    \"\"\"\n",
    "    z = norm.ppf(1 - alpha/2)\n",
    "    p = num_correct / num_samples  # proportion correct\n",
    "    denom = 1 + z**2 / num_samples\n",
    "    center = (p + z**2/(2*num_samples)) / denom\n",
    "    half = (z / denom) * np.sqrt(p*(1-p)/num_samples + z**2/(4*num_samples**2))\n",
    "    ci = (center - half, center + half)\n",
    "    return p, ci\n",
    "\n",
    "# x = number correct from ONE OOF prediction per sample (possibly after averaging probs across repeats)\n",
    "# n = total samples\n",
    "# acc = x/n; wilson_ci(x,n)\n",
    "\n",
    "# --- Optional: t-interval across repeats ---\n",
    "def repeat_mean_t_ci(scores, alpha=0.05):\n",
    "    scores = np.asarray(scores)\n",
    "    R = scores.size\n",
    "    assert R >= 2\n",
    "    m = scores.mean()\n",
    "    sd = scores.std(ddof=1)\n",
    "    se = sd / np.sqrt(R)\n",
    "    tcrit = t.ppf(1 - alpha/2, R-1)\n",
    "    return m, (m - tcrit*se, m + tcrit*se)\n",
    "\n",
    "\n",
    "wilson_ci(451, 618), 451/618"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a19faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(model_name='test_model', model_format='test_format', metric_name='test_metric', repeat_results=[RepeatResult(repeat_id=0, mean_score=0.8, std_score=0.05), RepeatResult(repeat_id=1, mean_score=0.85, std_score=0.03), RepeatResult(repeat_id=2, mean_score=0.82, std_score=0.04)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy test classes\n",
    "from dataclasses import dataclass\n",
    "import dataclasses\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RepeatResult:\n",
    "    \"\"\"Result from a single repeat (collection of folds)\"\"\"\n",
    "\n",
    "    repeat_id: int\n",
    "    mean_score: float\n",
    "    std_score: float\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Complete evaluation result for a model\"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    model_format: str\n",
    "    metric_name: str\n",
    "    repeat_results: List[RepeatResult]\n",
    "\n",
    "repeat_results = [\n",
    "    RepeatResult(repeat_id=id, mean_score=score, std_score=std)\n",
    "    for id, score, std in\n",
    "    zip(range(3), [0.8, 0.85, 0.82], [0.05, 0.03, 0.04])\n",
    "]\n",
    "\n",
    "evaluation_result = EvaluationResult(\n",
    "    model_name=\"test_model\",\n",
    "    model_format=\"test_format\",\n",
    "    metric_name=\"test_metric\",\n",
    "    repeat_results=repeat_results)\n",
    "\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5472da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'test_model',\n",
       " 'model_format': 'test_format',\n",
       " 'metric_name': 'test_metric',\n",
       " 'repeat_results': [{'repeat_id': 0, 'mean_score': 0.8, 'std_score': 0.05},\n",
       "  {'repeat_id': 1, 'mean_score': 0.85, 'std_score': 0.03},\n",
       "  {'repeat_id': 2, 'mean_score': 0.82, 'std_score': 0.04}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataclasses.asdict(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22eb1bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(model_name='test_model', model_format='test_format', metric_name='test_metric', repeat_results=[{'repeat_id': 0, 'mean_score': 0.8, 'std_score': 0.05}, {'repeat_id': 1, 'mean_score': 0.85, 'std_score': 0.03}, {'repeat_id': 2, 'mean_score': 0.82, 'std_score': 0.04}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct from dict\n",
    "EvaluationResult(**dataclasses.asdict(evaluation_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c145de8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.85])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75c227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.595, 0.605])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "repeat_scores = np.array([\n",
    "    [.67, .70],\n",
    "    [.70, .73],\n",
    "    [.48, .47],\n",
    "    [.53, .52]\n",
    "])\n",
    "\n",
    "repeat_scores.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78077ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat_mean_t_ci(scores): (0.8, (nan, nan))\n",
      "manual: 0.8, (nan, nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellisbrown/workspace/test-set-training/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/ellisbrown/workspace/test-set-training/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "def repeat_mean_t_ci(scores, alpha=0.05):\n",
    "    \"\"\"Repeat mean t-confidence interval\n",
    "    \n",
    "    Args:\n",
    "        scores: array of scores\n",
    "        alpha: significance level (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "        m: mean score\n",
    "        ci: tuple of lower and upper bounds of the t-confidence interval\n",
    "    \"\"\"\n",
    "    scores = np.asarray(scores)\n",
    "    m = scores.mean()\n",
    "    R = scores.size\n",
    "    # assert R >= 2\n",
    "    if R < 2:\n",
    "        return m, (np.nan, np.nan)\n",
    "    sd = scores.std(ddof=1)\n",
    "    se = sd / np.sqrt(R)\n",
    "    tcrit = t.ppf(1 - alpha/2, R-1)\n",
    "    return m, (m - tcrit*se, m + tcrit*se)\n",
    "\n",
    "\n",
    "# scores = np.array([0.8, 0.85, 0.82, 0.8, 0.83])\n",
    "scores = np.array([0.8])\n",
    "print(f\"repeat_mean_t_ci(scores): {repeat_mean_t_ci(scores)}\")\n",
    "\n",
    "score_macro_avg = scores.mean()\n",
    "score_macro_std = scores.std(ddof=1)  # sample SD (unbiased)\n",
    "score_macro_se = score_macro_std / np.sqrt(scores.shape[0])  # sample Std Error of the mean\n",
    "score_macro_t_95ci = t.interval(0.95, scores.shape[0] - 1, loc=score_macro_avg, scale=score_macro_se)\n",
    "print(f\"manual: {score_macro_avg}, {score_macro_t_95ci}\")\n",
    "\n",
    "# check if equal\n",
    "repeat_mean_t_ci(scores) == (score_macro_avg, score_macro_t_95ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26371179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a1e0b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_macro_t_95ci[0] == np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6196da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.82, (0.8039999999999999, 0.836))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "import numpy as np\n",
    "\n",
    "def bootstrap_mean_ci(values: Iterable[float], B=10_000, alpha=0.05, seed=0) -> tuple[float, tuple[float, float]]:\n",
    "    \"\"\"Bootstrap mean confidence interval\n",
    "\n",
    "    Args:\n",
    "        values: array of values\n",
    "        B: number of bootstrap samples (default: 10_000)\n",
    "        alpha: significance level (default: 0.05)\n",
    "        seed: random seed (default: 0)\n",
    "    \n",
    "    Returns:\n",
    "        m: mean of values\n",
    "        ci: tuple of lower and upper bounds of the bootstrap confidence interval\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    values = np.array(values)\n",
    "    n = len(values)\n",
    "    boots = np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        boots[b] = values[idx].mean()\n",
    "    lo, hi = np.percentile(boots, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return values.mean(), (lo, hi)\n",
    "\n",
    "values = np.array([0.8, 0.85, 0.82, 0.8, 0.83])\n",
    "bootstrap_mean_ci(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796dbcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BootstrapResult(confidence_interval=ConfidenceInterval(low=0.8039999999999999, high=0.836), bootstrap_distribution=array([0.808, 0.804, 0.81 , ..., 0.814, 0.828, 0.826]), standard_error=0.00853231052416974)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "spb = stats.bootstrap(values.reshape(1, -1), np.mean, confidence_level=0.95, method='basic')\n",
    "spb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3261fc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfidenceInterval(low=0.8039999999999999, high=0.836)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spb.confidence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "778cb4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00853231052416974"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spb.standard_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52b65526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008485281374238558"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.std(ddof=0) / np.sqrt(values.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41883451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 5\n",
      "idx: [4 3 2 1 1]\n",
      "values[idx]: [0.83 0.8  0.82 0.85 0.85]\n",
      "values[idx].mean(): 0.8299999999999998\n"
     ]
    }
   ],
   "source": [
    "values = np.array([0.8, 0.85, 0.82, 0.8, 0.83])\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "n = len(values)\n",
    "print(f\"n: {n}\")\n",
    "idx = rng.integers(0, n, size=n)\n",
    "print(f\"idx: {idx}\")\n",
    "print(f\"values[idx]: {values[idx]}\")\n",
    "print(f\"values[idx].mean(): {values[idx].mean()}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a18084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stratified Bootstrap CI (4 groups): 100%|██████████| 10000/10000 [00:00<00:00, 25391.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.73875, (0.72875, 0.74875))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any, Iterable\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def stratified_bootstrap_mean_ci(\n",
    "    groups: Iterable[Any],\n",
    "    values: Iterable[float],\n",
    "    B: int = 10_000,\n",
    "    alpha: float = 0.05,\n",
    "    seed: int = 0\n",
    ") -> tuple[float, tuple[float, float]]:\n",
    "    \"\"\"Stratified bootstrap mean confidence interval\n",
    "\n",
    "    Args:\n",
    "        groups: array of group labels. Shape: (n,)\n",
    "        values: array of values. Shape: (n,)\n",
    "        B: number of bootstrap samples (default: 10_000)\n",
    "        alpha: significance level (default: 0.05)\n",
    "        seed: random seed (default: 0)\n",
    "    \n",
    "    Returns:\n",
    "        m: mean of values\n",
    "        ci: tuple of lower and upper bounds of the stratified bootstrap confidence interval\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    groups = np.asarray(groups)\n",
    "    values = np.asarray(values)\n",
    "\n",
    "    assert len(groups) == len(values), (\n",
    "        f\"groups and values must have the same length, \"\n",
    "        f\"got {len(groups)} and {len(values)}\"\n",
    "    )\n",
    "\n",
    "    uniq = np.unique(groups)\n",
    "    boots = np.empty(B)\n",
    "    \n",
    "    for b in trange(B, desc=f\"Stratified Bootstrap CI ({len(uniq)} groups)\"):\n",
    "        parts = []\n",
    "        # resample within each group (stratum)\n",
    "        for g in uniq:\n",
    "            idx = np.where(groups == g)[0]  # indices for this group\n",
    "            bs = rng.choice(idx, size=len(idx), replace=True)  # n random indices w/ replacement\n",
    "            parts.append(values[bs])\n",
    "        boots[b] = np.concatenate(parts).mean()  # combine groups and compute mean\n",
    "    \n",
    "    # compute confidence interval from bootstrap distribution\n",
    "    lo, hi = np.percentile(boots, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return values.mean(), (lo, hi)\n",
    "\n",
    "\n",
    "groups = np.array([0, 0, 1, 1, 1, 2, 3, 3])\n",
    "values = np.array([0.8, 0.85, 0.62, 0.6, 0.63, 0.91, 0.74, 0.76])\n",
    "stratified_bootstrap_mean_ci(groups, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d70f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
