
@inproceedings{lei-etal-2023-revealing,
    title = "Revealing Single Frame Bias for Video-and-Language Learning",
    author = "Lei, Jie  and
      Berg, Tamara  and
      Bansal, Mohit",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "ACL",
    year = "2023",
    url = "https://aclanthology.org/2023.acl-long.29",
    doi = "10.18653/v1/2023.acl-long.29",
    abstract = "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong {``}static appearance bias{''} in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at \url{https://github.com/jayleicn/singularity}.",
}

@article{xu2024mcbenchbenchmarkmulticontextvisual,
  title={MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs},
  author={Xu, Yunqiu and Zhu, Linchao and Yang, Yi},
  journal={arXiv preprint arXiv:2410.12332},
  year={2024}
}

@inproceedings{dai2017scannet,
  title={Scannet: Richly-annotated 3d reconstructions of indoor scenes},
  author={Dai, Angela and Chang, Angel X and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{yeshwanth2023scannet++,
  title={Scannet++: A high-fidelity dataset of 3d indoor scenes},
  author={Yeshwanth, Chandan and Liu, Yueh-Cheng and Nie{\ss}ner, Matthias and Dai, Angela},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{
dehghan2021arkitscenes,
title={{ARK}itScenes - A Diverse Real-World Dataset for 3D Indoor Scene Understanding Using Mobile {RGB}-D Data},
author={Gilad Baruch and Zhuoyuan Chen and Afshin Dehghan and Tal Dimry and Yuri Feigin and Peter Fu and Thomas Gebauer and Brandon Joffe and Daniel Kurz and Arik Schwartz and Elad Shulman},
booktitle={NeurIPS},
year={2021},
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{huang2023can,
  title={Can large language models explain themselves? a study of llm-generated self-explanations},
  author={Huang, Shiyuan and Mamidanna, Siddarth and Jangam, Shreedhar and Zhou, Yilun and Gilpin, Leilani H},
  journal={arXiv preprint arXiv:2310.11207},
  year={2023}
}

@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={IJCV},
  year={2010}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014}
}

@book{salton1986introduction,
author = {Salton, Gerard and McGill, Michael J.},
title = {Introduction to Modern Information Retrieval},
year = {1986},
publisher = {McGraw-Hill, Inc.},
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={NeurIPS},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={NeurIPS},
  year={2022}
}

@book{gardner1983frames,
  author       = {Howard Gardner},
  title        = {Frames of Mind: The Theory of Multiple Intelligences},
  year         = {1983},
  edition      = {Tenth-anniversary Edition, Second Paperback Edition},
  publisher    = {Basic Books}
}

@article{naveed2024comprehensiveoverviewlargelanguage,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  journal={arXiv preprint arXiv:2307.06435},
  year={2023}
}


@article{beguvs2023large,
  title={Large linguistic models: Analyzing theoretical linguistic abilities of LLMs},
  author={Begu{\v{s}}, Ga{\v{s}}per and D{\k{a}}bkowski, Maksymilian and Rhodes, Ryan},
  journal={arXiv preprint arXiv:2305.00948},
  year={2023}
}

@inproceedings{zhang-etal-2024-unveiling-linguistic,
    title = "Unveiling Linguistic Regions in Large Language Models",
    author = "Zhang, Zhihao  and
      Zhao, Jun  and
      Zhang, Qi  and
      Gui, Tao  and
      Huang, Xuanjing",
    booktitle = "ACL",
    year = "2024"
}


@inproceedings{Kassner2023LanguageMW,
  title={Language Models with Rationality},
  author={Nora Kassner and Oyvind Tafjord and Ashish Sabharwal and Kyle Richardson and Hinrich Sch{\"u}tze and Peter Clark},
  booktitle={EMNLP},
  year={2023}
}


@article{clark1991dual,
  author    = {Clark, James M. and Paivio, Allan},
  title     = {Dual Coding Theory and Education},
  journal   = {Educational Psychology Review},
  year      = {1991},
}


@article{meneghetti2022individual,
  title={Individual differences in navigation: an introductory overview},
  author={Meneghetti, Chiara and Miola, Laura and Feraco, Tommaso and Muffato, Veronica and Miola, Tommaso Feraco},
  journal={Prime archives in psychology},
  year={2022},
  publisher={Vide Leaf Hyderabad}
}


@inbook{Newcombe2024Spatial,
	author = {Newcombe, Nora S.},
	booktitle = {Open {Encyclopedia} of {Cognitive} {Science}},
	editor = {Frank, Michael C. and Majid, Asifa},
	year = {2024},
	month = {jul 24},
	note = {https://oecs.mit.edu/pub/or750iar},
	publisher = {MIT Press},
	title = {Spatial {Cognition}},
}

@article{chabris_jerde_woolley_gerbasi_schuldt_wai_bennett_hackman_kosslyn_2023,
  title={Spatial and object visualization cognitive styles: Validation studies in 3800 individuals},
  author={Chabris, Christopher F and Jerde, Thomas E and Woolley, Anita W and Gerbasi, Margaret E and Schuldt, Jonathon P and Bennett, Sean L and Hackman, J Richard and Kosslyn, Stephen M},
  journal={Group brain technical report},
  year={2006}
}

@article{tolman1948cognitive,
  author    = {Tolman, E. C.},
  title     = {Cognitive maps in rats and men},
  journal   = {Psychological Review},
  year      = {1948},
  doi       = {10.1037/h0061626}
}

@article{mcafoose2009exploring,
  author    = {McAfoose, Julia and Baune, Bernhard T.},
  title     = {Exploring Visual-Spatial Working Memory: A Critical Review of Concepts and Models},
  journal   = {Neuropsychology Review},
  year      = {2009},
}

@book{dehn2011working,
  author    = {Dehn, Milton J.},
  title     = {Working Memory and Academic Learning: Assessment and Intervention},
  year      = {2011},
  publisher = {John Wiley \& Sons},
}

@article{hurst2024gpto,
  title={GPT-4o System Card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle={CVPR},
  year={2024}
}

@article{xue2024longvila,
  title={Longvila: Scaling long-context visual language models for long videos},
  author={Xue, Fuzhao and Chen, Yukang and Li, Dacheng and Hu, Qinghao and Zhu, Ligeng and Li, Xiuyu and Fang, Yunhao and Tang, Haotian and Yang, Shang and Liu, Zhijian and others},
  journal={arXiv preprint arXiv:2408.10188},
  year={2024}
}

@article{li2024llavaov,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@misc{zhang2024llavanextvideo,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  year={2024}
}

@inproceedings{
    jimenez2024swebench,
    title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
    author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
    booktitle={ICLR},
    year={2024},
}

@inproceedings{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{wang2022self,
    title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
    author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
    booktitle={ICLR},
    year={2023},
}

@inproceedings{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  booktitle={NeurIPS},
  year={2024}
}

@article{schulman2022chatgpt,
  title={{ChatGPT: Optimizing language models for dialogue}},
  author={Schulman, John and Zoph, Barret and Kim, Christina and Hilton, Jacob and Menick, Jacob and Weng, Jiayi and Uribe, Juan Felipe Ceron and Fedus, Liam and Metz, Luke and Pokorny, Michael and others},
  journal={OpenAI blog},
  year={2022}
}

@inproceedings{tian2024drivevlm,
  title={Drivevlm: The convergence of autonomous driving and large vision-language models},
  author={Tian, Xiaoyu and Gu, Junru and Li, Bailin and Liu, Yicheng and Wang, Yang and Zhao, Zhiyong and Zhan, Kun and Jia, Peng and Lang, Xianpeng and Zhao, Hang},
  booktitle={CoRL},
  year={2024}
}

@inproceedings{shao2024lmdrive,
  title={Lmdrive: Closed-loop end-to-end driving with large language models},
  author={Shao, Hao and Hu, Yuxuan and Wang, Letian and Song, Guanglu and Waslander, Steven L and Liu, Yu and Li, Hongsheng},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{brohan2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  booktitle={CoRL},
  year={2023}
}

@inproceedings{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  booktitle={RSS},
  year={2023}
}

@article{o2023open,
  title={Open x-embodiment: Robotic learning datasets and rt-x models},
  author={O'Neill, Abby and Rehman, Abdul and Gupta, Abhinav and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and others},
  journal={arXiv preprint arXiv:2310.08864},
  year={2023}
}

@article{wang2023voyager,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={TMLR},
  year={2023}
}

@book{nadel2008hippocampus,
  author    = {Nadel, Lynn},
  title     = {The Hippocampus and Context Revisited},
  year      = {2008},
  publisher = {Oxford University Press},
  doi       = {10.1093/acprof:oso/9780195323245.001.0001},
  isbn      = {978-0-19-986926-8}
}

@inproceedings{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={ICML},
  year={2022}
}

@inproceedings{zhang-etal-2023-video,
    title = "Video-{LL}a{MA}: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
    author = "Zhang, Hang  and
      Li, Xin  and
      Bing, Lidong",
    editor = "Feng, Yansong  and
      Lefever, Els",
    booktitle = "EMNLP",
    year = "2023",
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={TMLR},
  year={2022}
}

@article{2023videochat,
  title={VideoChat: Chat-Centric Video Understanding},
  author={Li, Kunchang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@InProceedings{Ren_2024_CVPR,
    author    = {Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
    title     = {TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding},
    booktitle = {CVPR},
    year={2024}
}

@inproceedings{huang2024vtimellm,
  title={Vtimellm: Empower llm to grasp video moments},
  author={Huang, Bin and Wang, Xin and Chen, Hong and Song, Zihan and Zhu, Wenwu},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{caba2015activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={CVPR},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{cheng2024spatialrgpt,
      title={SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models},
      author={Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei},
      booktitle={NeurIPS},
      year={2024}
}

@article{cai2024spatialbot,
  title={SpatialBot: Precise Spatial Understanding with Vision Language Models},
  author={Cai, Wenxiao and Ponomarenko, Yaroslav and Yuan, Jianhao and Li, Xiaoqi and Yang, Wankou and Dong, Hao and Zhao, Bo},
  journal={arXiv preprint arXiv:2406.13642},
  year={2024}
}

@article{liu2024coarsecorrespondenceelicit3d,
  title={Coarse correspondence elicit 3d spacetime understanding in multimodal language model},
  author={Liu, Benlin and Dong, Yuhao and Wang, Yiqin and Rao, Yongming and Tang, Yansong and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2408.00754},
  year={2024}
}

@article{yang2023setofmarkpromptingunleashesextraordinary,
      title={Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V}, 
      author={Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao},
      journal={arXiv preprint arXiv:2310.11441},
      year={2023},
}

@inproceedings{fu2024mmecomprehensiveevaluationbenchmark,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yongdong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  booktitle={CVPR},
  year={2025}
}


@inproceedings{yue2023mmmu,
    title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
    author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
    booktitle={CVPR},
    year={2024},
}

@inproceedings{li-etal-2024-multimodal-arxiv,
  title = "Multimodal {A}r{X}iv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models",
  author = "Li, Lei  and
    Wang, Yuqi  and
    Xu, Runxin  and
    Wang, Peiyi  and
    Feng, Xiachong  and
    Kong, Lingpeng  and
    Liu, Qi",
  editor = "Ku, Lun-Wei  and
    Martins, Andre  and
    Srikumar, Vivek",
  booktitle = "ACL",
  year = "2024",
  url = "https://aclanthology.org/2024.acl-long.775",
  doi = "10.18653/v1/2024.acl-long.775"
}

@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@InProceedings{Li_2024_CVPR,
    author    = {Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and Wang, Limin and Qiao, Yu},
    title     = {MVBench: A Comprehensive Multi-modal Video Understanding Benchmark},
    booktitle = {CVPR},
    month     = {June},
    year      = {2024},
}

@article{ye2024mmegobuildingegocentricmultimodal,
  title={MM-Ego: Towards Building Egocentric Multimodal LLMs},
  author={Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2410.07177},
  year={2024}
}


@inproceedings{liu-etal-2024-tempcompass,
    title = "{T}emp{C}ompass: Do Video {LLM}s Really Understand Videos?",
    author = "Liu, Yuanxin  and
      Li, Shicheng  and
      Liu, Yi  and
      Wang, Yuxiang  and
      Ren, Shuhuai  and
      Li, Lei  and
      Chen, Sishuo  and
      Sun, Xu  and
      Hou, Lu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of ACL",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-acl.517",
    doi = "10.18653/v1/2024.findings-acl.517",
}

@inproceedings{li2024vitatecsdiagnosticdatasettemporal,
  title={Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models},
  author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{chen2024rextimebenchmarksuitereasoningacrosstime,
  title={ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos},
  author={Chen, Jr-Jen and Liao, Yu-Chien and Lin, Hsi-Che and Yu, Yu-Chu and Chen, Yen-Chun and Wang, Yu-Chiang Frank},
  booktitle={NeurIPS},
  year={2024}
}


@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={KDD},
  year={2016}
}

@article{xu2024drivegpt4,
  title={Drivegpt4: Interpretable end-to-end autonomous driving via large language model},
  author={Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kwan-Yee K and Li, Zhenguo and Zhao, Hengshuang},
  journal={RA-L},
  year={2024},
  publisher={IEEE}
}

@inproceedings{chandrasegaran2024hourvideo,
      title={HourVideo: 1-Hour Video-Language Understanding},
      author={Chandrasegaran, Keshigeyan and Gupta, Agrim and Hadzic, Lea M. and Kota, Taran and 
      He, Jimming and Eyzaguirre, Cristobal and Durante, Zane and Li, Manling and Wu, Jiajun and Li, Fei-Fei},
      booktitle = {NeurIPS},
      year={2024},
      volume = {37},
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={NeurIPS},
  year={2023}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{gur2023real,
  title={A real-world webagent with planning, long context understanding, and program synthesis},
  author={Gur, Izzeddin and Furuta, Hiroki and Huang, Austin and Safdari, Mustafa and Matsuo, Yutaka and Eck, Douglas and Faust, Aleksandra},
  booktitle={ICLR},
  year={2024}
}

@article{shepard1988mental,
  title={Mental rotation: effects of dimensionality of objects and type of task.},
  author={Shepard, Shenna and Metzler, Douglas},
  journal={Journal of experimental psychology: Human perception and performance},
  volume={14},
  number={1},
  year={1988},
  publisher={American Psychological Association}
}


@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  journal={OpenAI Blog},
  year={2018}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  journal={NeurIPS},
  year={2020}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{bai2023qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={CVPR},
  year={2024}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hu2021lora,
  title={{LoRA: Low-Rank Adaptation of Large Language Models}},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={ICLR},
  year={2022}
}

@article{schulman2025lora,
  author = {John Schulman and Thinking Machines Lab},
  title = {{LoRA Without Regret}},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/lora/},
  doi = {10.64434/tml.20250929},
}

@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  year={2023}
}

@article{mu2024embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal={NeurIPS},
  year={2024}
}

@article{liu2024world,
  title={World model on million-length video and language with ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2402.08268},
  year={2024}
}

@inproceedings{kim2024openvla,
  title={OpenVLA: An Open-Source Vision-Language-Action Model},
  author={Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and others},
  booktitle={CoRL},
  year={2024}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023}
}

@inproceedings{chen2024spatialvlm,
  title={Spatialvlm: Endowing vision-language models with spatial reasoning capabilities},
  author={Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brain and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle={CVPR},
  year={2024}
}


@inproceedings{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  booktitle={ICML},
  year={2023}
}

@inproceedings{yue2024mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{liu2025mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={ECCV},
  year={2025}
}

@inproceedings{li2024seed,
  title={SEED-Bench: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={CVPR},
  year={2024}
}

@article{yu2023mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={ICML},
  year={2024}
}


@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}

@article{ning2023videobench,
  title={Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models},
  author={Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
  journal={arXiv preprint arXiv:2311.16103},
  year={2023}
}


@article{li2023vitatecs,
  title={Vitatecs: A diagnostic dataset for temporal concept understanding of video-language models},
  author={Li, Shicheng and Li, Lei and Ren, Shuhuai and Liu, Yuanxin and Liu, Yi and Gao, Rundong and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2311.17404},
  year={2023}
}

@inproceedings{fang2024mmbenchvideo,
  title={MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  booktitle={NeurIPS},
  year={2024}
}



@article{li2024topviewrs,
  title={TopViewRS: Vision-Language Models as Top-View Spatial Reasoners},
  author={Li, Chengzu and Zhang, Caiqi and Zhou, Han and Collier, Nigel and Korhonen, Anna and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:2406.02537},
  year={2024}
}

@article{tang2024sparkle,
  title={Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning},
  author={Tang, Yihong and Qu, Ao and Wang, Zhaokai and Zhuang, Dingyi and Wu, Zhaofeng and Ma, Wei and Wang, Shenhao and Zheng, Yunhan and Zhao, Zhan and Zhao, Jinhua},
  journal={arXiv preprint arXiv:2410.16162},
  year={2024}
}

@inproceedings{yang2024virl,
  title={V-irl: Grounding virtual intelligence in real life},
  author={Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining},
  booktitle={ECCV},
  year={2024}
}

@article{ramakrishnan2024does,
  title={Does Spatial Cognition Emerge in Frontier Models?},
  author={Ramakrishnan, Santhosh Kumar and Wijmans, Erik and Kraehenbuehl, Philipp and Koltun, Vladlen},
  journal={arXiv preprint arXiv:2410.06468},
  year={2024}
}


@article{zhu2024llava3d,
  title={LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness},
  author={Zhu, Chenming and Wang, Tai and Zhang, Wenwei and Pang, Jiangmiao and Liu, Xihui},
  journal={arXiv preprint arXiv:2409.18125},
  year={2024}
}

@article{wu2024visualization,
  title={Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models},
  author={Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
  journal={NeurIPS},
  year={2024}
}

@article{buhner2008working,
  title={Working memory, visual--spatial-intelligence and their relationship to problem-solving},
  author={B{\"u}hner, Markus and Kr{\"o}ner, Stephan and Ziegler, Matthias},
  journal={Intelligence},
  volume={36},
  number={6},
  year={2008},
  publisher={Elsevier}
}

@article{marshall2001spatial,
  title={Spatial cognition: where we were and where we are},
  author={Marshall, John C and Fink, Gereon R},
  journal={Neuroimage},
  volume={14},
  number={1},
  year={2001},
  publisher={Elsevier}
}

@book{waller2013handbook,
  title={Handbook of spatial cognition.},
  author={Waller, David Ed and Nadel, Lynn Ed},
  year={2013},
  publisher={American Psychological Association}
}

@book{mallot2024geometry,
  title={From geometry to behavior: An introduction to spatial cognition},
  author={Mallot, Hanspeter A},
  year={2024},
  publisher={MIT Press}
}

@article{vasilyeva2012development,
  title={Development of spatial cognition},
  author={Vasilyeva, Marina and Lourenco, Stella F},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={3},
  number={3},
  year={2012},
  publisher={Wiley Online Library}
}
@book{newcombe2000making,
  title={Making space: The development of spatial representation and reasoning},
  author={Newcombe, NS},
  year={2000},
  publisher={MIT Press}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={PAMI},
  volume={39},
  number={6},
  year={2016},
  publisher={IEEE}
}

@article{
yamada2024evaluating,
title={Evaluating Spatial Understanding of Large Language Models},
author={Yutaro Yamada and Yihan Bao and Andrew Kyle Lampinen and Jungo Kasai and Ilker Yildirim},
journal={TMLR},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=xkiflfKCw3},
note={}
}

@article{momennejad2024evaluating,
  title={Evaluating cognitive maps and planning in large language models with CogEval},
  author={Momennejad, Ida and Hasanbeig, Hosein and Vieira Frujeri, Felipe and Sharma, Hiteshi and Jojic, Nebojsa and Palangi, Hamid and Ness, Robert and Larson, Jonathan},
  journal={NeurIPS},
  year={2024}
}

@article{shepard1978mental,
  title={The mental image.},
  author={Shepard, Roger N},
  journal={American psychologist},
  volume={33},
  number={2},
  year={1978},
  publisher={American Psychological Association}
}

@article{rajabi2023towards,
  title={Towards grounded visual spatial reasoning in multi-modal vision language models},
  author={Rajabi, Navid and Kosecka, Jana},
  journal={arXiv preprint arXiv:2308.09778},
  year={2023}
}

@article{liu2023visual,
  title={Visual spatial reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={TACL},
  year={2023},
  publisher={MIT Press}
}

@inproceedings{liu-etal-2022-things,
    title = "Things not Written in Text: Exploring Spatial Commonsense from Visual Signals",
    author = "Liu, Xiao  and
      Yin, Da  and
      Feng, Yansong  and
      Zhao, Dongyan",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "ACL",
    year = "2022",
    url = "https://aclanthology.org/2022.acl-long.168",
    doi = "10.18653/v1/2022.acl-long.168",
}

@inproceedings{mirzaee-etal-2021-spartqa,
    title = "{SPARTQA}: A Textual Question Answering Benchmark for Spatial Reasoning",
    author = "Mirzaee, Roshanak  and
      Rajaby Faghihi, Hossein  and
      Ning, Qiang  and
      Kordjamshidi, Parisa",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "NAACL",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.364",
    doi = "10.18653/v1/2021.naacl-main.364",
}


@inproceedings{collell2018acquiring,
  title={Acquiring common sense spatial knowledge through implicit spatial templates},
  author={Collell, Guillem and Van Gool, Luc and Moens, Marie-Francine},
  booktitle={AAAI},
  volume={32},
  number={1},
  year={2018}
}

@incollection{freksa1991qualitative,
  title={Qualitative spatial reasoning},
  author={Freksa, Christian},
  booktitle={Cognitive and linguistic aspects of geographic space},
  year={1991}
}

@inproceedings{cho2023spatially,
  title={Spatially-Aware Transformers for Embodied Agents},
  author={Cho, Junmo and Yoon, Jaesik and Ahn, Sungjin},
  booktitle={ICLR},
  year={2023}
}

@article{rozanova2021grounding,
  title={Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?},
  author={Rozanova, Julia and Ferreira, Deborah and Dubba, Krishna and Cheng, Weiwei and Zhang, Dell and Freitas, Andre},
  journal={arXiv preprint arXiv:2109.08634},
  year={2021}
}

@article{chen2024internvl2,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{zhang2024longva,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={ICLR},
year={2021},
url={https://openreview.net/forum?id=d7KBjmI3GmQ}
}

@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={CVPR},
  year={2024}
}

@article{baddeley1992working,
  title={Working memory},
  author={Baddeley, Alan},
  journal={Science},
  volume={255},
  number={5044},
  year={1992}
}

@inproceedings{parcalabescu2024measuring,
  title={On measuring faithfulness or self-consistency of natural language explanations},
  author={Parcalabescu, Letitia and Frank, Anette},
  booktitle={ACL},
  year={2024}
}


@inproceedings{lyu-etal-2023-faithful,
    title = "Faithful Chain-of-Thought Reasoning",
    author = "Lyu, Qing  and
      Havaldar, Shreya  and
      Stein, Adam  and
      Zhang, Li  and
      Rao, Delip  and
      Wong, Eric  and
      Apidianaki, Marianna  and
      Callison-Burch, Chris",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "ACL",
    year = "2023"
}


@inproceedings{gao-etal-2024-self,
    title = "Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models",
    author = "Gao, Haoyu  and
      Lin, Ting-En  and
      Li, Hangyu  and
      Yang, Min  and
      Wu, Yuchuan  and
      Ma, Wentao  and
      Huang, Fei  and
      Li, Yongbin",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "COLING",
    year = "2024",
}


@article{
oquab2024dinov,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal={TMLR},
year={2024},
}

@article{Zhou2018open3d,
   author  = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},
   title   = {{Open3D}: {A} Modern Library for {3D} Data Processing},
   journal = {arXiv:1801.09847},
   year    = {2018},
}

@article{zhang2024lmms,
  title={Lmms-eval: Reality check on the evaluation of large multimodal models},
  author={Zhang, Kaichen and Li, Bo and Zhang, Peiyuan and Pu, Fanyi and Cahyono, Joshua Adrian and Hu, Kairui and Liu, Shuai and Zhang, Yuanhan and Yang, Jingkang and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2407.12772},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{yang2024think,
    title={{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},
    author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali W. and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
    journal={CVPR},
    year={2024},
}

@article{tong2024cambrian,
  title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},
  journal={NeurIPS},
  year={2024}
}

@article{fu2024blink,
    title={{BLINK: Multimodal Large Language Models Can See but Not Perceive}},
    author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
    journal={ECCV},
    year={2024}
}
    
@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}
@article{stone1974cross,
  title={Cross-validatory choice and assessment of statistical predictions},
  author={Stone, Mervyn},
  journal={Journal of the royal statistical society},
  year={1974},
  publisher={Wiley Online Library}
}
@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  year={2001},
  publisher={Springer}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  year={2001},
  publisher={JSTOR}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{yue2024mmmu,
  title={MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark},
  author={Xiang Yue and Tianyu Zheng and Yuansheng Ni and Yubo Wang and Kai Zhang and Shengbang Tong and Yuxuan Sun and Botao Yu and Ge Zhang and Huan Sun and Yu Su and Wenhu Chen and Graham Neubig},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}

@inproceedings{goyal2017making,
  title={{Making the V in VQA matter: Elevating the role of image understanding in visual question answering}},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@article{nembrini2018revival,
  title={The revival of the Gini importance?},
  author={Nembrini, Stefano and K{\"o}nig, Inke R and Wright, Marvin N},
  journal={Bioinformatics},
  year={2018},
  publisher={Oxford University Press}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  year={1998},
  publisher={IEEE}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex},
  year={2009},
  institution={University of Toronto}
}

@inproceedings{deng2009imagenet,
  title={ImageNet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009},
}

@inproceedings{antol2015vqa,
  title={{VQA: Visual Question Answering}},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{kay2017kinetics,
  title={The Kinetics Human Action Video Dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  booktitle={arXiv preprint arXiv:1705.06950},
  year={2017}
}


@inproceedings{gupta2019lvis,
  title={Lvis: A dataset for large vocabulary instance segmentation},
  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}

@article{yang2023dawn,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  year={2023}
}

@article{yuan2021language,
  title={Language bias in visual question answering: A survey and taxonomy},
  author={Yuan, Desen},
  journal={arXiv preprint arXiv:2111.08531},
  year={2021}
}

@inproceedings{cho2023generative,
  title={Generative bias for robust visual question answering},
  author={Cho, Jae Won and Kim, Dong-Jin and Ryu, Hyeonggon and Kweon, In So},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{manjunatha2019explicit,
  title={Explicit bias discovery in visual question answering models},
  author={Manjunatha, Varun and Saini, Nirat and Davis, Larry S},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{liang2021lpf,
  title={LPF: A language-prior feedback objective function for de-biased visual question answering},
  author={Liang, Zujie and Hu, Haifeng and Zhu, Jiaying},
  booktitle={SIGIR},
  year={2021}
}

@inproceedings{niu2021counterfactual,
  title={Counterfactual vqa: A cause-effect look at language bias},
  author={Niu, Yulei and Tang, Kaihua and Zhang, Hanwang and Lu, Zhiwu and Hua, Xian-Sheng and Wen, Ji-Rong},
  booktitle={CVPR},
  year={2021}
}

@article{si2021check,
  title={Check it again: Progressive visual question answering via visual entailment},
  author={Si, Qingyi and Lin, Zheng and Zheng, Mingyu and Fu, Peng and Wang, Weiping},
  journal={arXiv preprint arXiv:2106.04605},
  year={2021}
}

@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={CVPR},
  year={2018}
}

@article{cadene2019rubi,
  title={{RUBi}: Reducing unimodal biases for visual question answering},
  author={Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  journal={NeurIPS},
  year={2019}
}

@article{wen2021debiased,
  title={Debiased visual question answering from feature and sample perspectives},
  author={Wen, Zhiquan and Xu, Guanghui and Tan, Mingkui and Wu, Qingyao and Wu, Qi},
  journal={NeurIPS},
  year={2021}
}

@inproceedings{ramakrishnan2018overcoming,
  title={Overcoming Language Priors in Visual Question Answering with Adversarial Regularization},
  author={Ramakrishnan, Sainandan and Agrawal, Aishwarya and Lee, Stefan},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{abbasnejad2020counterfactual,
  title={Counterfactual Vision and Language Learning},
  author={Abbasnejad, Ehsan and Teney, Damien and Parvaneh, Amin and Shi, Javen and van den Hengel, Anton},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{park2024modality,
  title={Assessing Modality Bias in Video Question Answering Benchmarks with Multimodal Large Language Models},
  author={Park, Jean and Jang, Kuk Jin and Alasaly, Basam and Mopidevi, Sriharsha and Zolensky, Andrew and Eaton, Eric and Lee, Insup and Johnson, Kevin},
  booktitle={AAAI},
  year={2025}
}

@inproceedings{agarwal2024rci,
  title={RCI: A Score for Evaluating Global and Local Reasoning in Multimodal Benchmarks},
  author={Agarwal, Amit and Patel, Hitesh Laxmichand and Panda, Srikant and Meghwani, Hansa and Singh, Jyotika and Dua, Karan and Li, Paul and Sheng, Tao and Ravi, Sujith and Roth, Dan},
  booktitle={EMNLP},
  year={2025}
}

@infproceedings{chen2024quantifying,
  title={Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective},
  author={Chen, Meiqi and Cao, Yixin and Zhang, Yan and Lu, Chaochao},
  booktitle={EMNLP},
  year={2024}
}


@inproceedings{thrush2022winoground,
  author = {Tristan Thrush and Ryan Jiang and Max Bartolo and Amanpreet Singh and Adina Williams and Douwe Kiela and Candace Ross},
  title = {Winoground: Probing vision and language models for visio-linguistic compositionality},
  booktitle = {CVPR},
  year = 2022,
}


@article{zhang2024vinoground,
  title={Vinoground: Scrutinizing lmms over dense temporal reasoning with short videos},
  author={Zhang, Jianrui and Cai, Mu and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2410.02763},
  year={2024}
}

@inproceedings{kiela2020hateful,
  title={The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{li2024seed,
  title={Seed-bench: Benchmarking multimodal large language models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{liang2024survey,
  title={A Survey of Multimodel Large Language Models},
  author={Liang, Zijing and Xu, Yanjie and Hong, Yifan and Shang, Penghui and Wang, Qi and Fu, Qiang and Liu, Ke},
  booktitle={CIACE},
  year={2024}
}

@inproceedings{liu2024mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  booktitle={ECCV},
  year={2024},
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={WACV},
  year={2021}
}

@article{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}

@article{liu2024ocrbench,
  title={OCRBench: on the hidden mystery of OCR in large multimodal models},
  author={Liu, Yuliang and Li, Zhang and Huang, Mingxin and Yang, Biao and Yu, Wenwen and Li, Chunyuan and Yin, Xu-Cheng and Liu, Cheng-Lin and Jin, Lianwen and Bai, Xiang},
  journal={Science China Information Sciences},
  year={2024},
}

@inproceedings{fu2024blink,
  title={Blink: Multimodal large language models can see but not perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  booktitle={ECCV},
  year={2024},
}

@article{chen2024we,
  title={Are we on the right way for evaluating large vision-language models?},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2403.20330},
  year={2024}
}

@article{liu2024decade,
  title={A Decade's Battle on Dataset Bias: Are We There Yet?},
  author={Liu, Zhuang and He, Kaiming},
  journal={arXiv preprint arXiv:2403.08632},
  year={2024}
}

@article{hermann2023foundations,
  title={On the foundations of shortcut learning},
  author={Hermann, Katherine L and Mobahi, Hossein and Fel, Thomas and Mozer, Michael C},
  journal={arXiv preprint arXiv:2310.16228},
  year={2023}
}

@article{li2025critical,
  title={A Critical Review of Predominant Bias in Neural Networks},
  author={Li, Jiazhi and Khayatkhoei, Mahyar and Zhu, Jiageng and Xie, Hanchen and Hussein, Mohamed E and AbdAlmageed, Wael},
  journal={arXiv preprint arXiv:2502.11031},
  year={2025}
}

@article{zeng2024understanding,
  title={Understanding Bias in Large-Scale Visual Datasets},
  author={Zeng, Boya and Yin, Yida and Liu, Zhuang},
  journal={arXiv preprint arXiv:2412.01876},
  year={2024}
}

@article{du2021towards,
  title={Towards interpreting and mitigating shortcut learning behavior of NLU models},
  author={Du, Mengnan and Manjunatha, Varun and Jain, Rajiv and Deshpande, Ruchi and Dernoncourt, Franck and Gu, Jiuxiang and Sun, Tong and Hu, Xia},
  journal={arXiv preprint arXiv:2103.06922},
  year={2021}
}

@inproceedings{parmar2018review,
  title={A review on random forest: An ensemble classifier},
  author={Parmar, Aakash and Katariya, Rakesh and Patel, Vatsal},
  booktitle={IDCIoT},
  year={2018},
}

@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{schaeffer2023pretraining,
  title={Pretraining on the test set is all you need},
  author={Schaeffer, Rylan},
  journal={arXiv preprint arXiv:2309.08632},
  year={2023}
}

@article{lu2022sqa,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={NeurIPS},
  year={2022}
}

@article{lu2023mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={ECCV},
  year={2016}
}

@article{zhang2024video,
  title={Video instruction tuning with synthetic data},
  author={Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2410.02713},
  year={2024}
}


@book{zipf1932selected,
  url = {https://doi.org/10.4159/harvard.9780674434929},
  title = {Selected Studies of the Principle of Relative Frequency in Language},
  author = {Zipf, George Kingsley},
  publisher = {Harvard University Press},
  address = {Cambridge, MA and London, England},
  doi = {doi:10.4159/harvard.9780674434929},
  year = {1932},
}



@article{yang2025cambrian-s,
    author = {Yang, Shusheng and Yang, Jihan and Huang, Pinzhi and Brown, Ellis and Yang, Zihao and Yu, Yue and Tong, Shengbang and Zheng, Zihan and Xu, Yifan and Wang, Muhan and Lu, Danhao and Fergus, Rob and LeCun, Yann and Fei-Fei, Li and Xie, Saining},
    title = {{Cambrian-S: Towards Spatial Supersensing in Video}},
    journal = {arXiv preprint},
    year = {2025}
}
