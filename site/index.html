<!doctype html>
<html lang="en">
    <head>
        <title>TsT: Test-Set Stress-Test</title>
        <link rel="icon" type="image/x-icon" href="static/img/header.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://vision-x-nyu.github.io/test-set-training/" />
        <meta property="og:image" content="https://vision-x-nyu.github.io/test-set-training/static/img/header.png" />
        <meta property="og:title" content="Test-Set Stress-Test (TsT)" />
        <meta property="og:description" content="'Train on the Test Set' to expose and mitigate non-visual shortcuts in multimodal benchmarks." />

        <!-- Twitter -->
        <meta name="twitter:url" content="https://vision-x-nyu.github.io/test-set-training/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://vision-x-nyu.github.io/test-set-training/static/img/header.png" />
        <meta name="twitter:title" content="Test-Set Stress-Test (TsT)" />
        <meta name="twitter:description" content="'Train on the Test Set' to expose and mitigate non-visual shortcuts in multimodal benchmarks." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>

        <link rel="preload" as="image" href="static/img/header.png" fetchpriority="high">
        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- medium zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
        <script>
            function toggleAnswer(answerId, element) {
                var answerDiv = document.getElementById(answerId);
                if (answerDiv.style.display === "none" || answerDiv.style.display === "") {
                    answerDiv.style.display = "block";
                    element.textContent = "Click to hide answer";
                } else {
                    answerDiv.style.display = "none";
                    element.textContent = "Click to view Ground Truth";
                }
            }
            
            // Set default playback speed to 2x for video_46
            document.addEventListener('DOMContentLoaded', function() {
                var video46 = document.getElementById('video_46');
                if (video46) {
                    video46.addEventListener('loadedmetadata', function() {
                        this.playbackRate = 2.0;
                    });
                    // Also set it immediately if video is already loaded
                    if (video46.readyState >= 1) {
                        video46.playbackRate = 2.0;
                    }
                }
            });
        </script>
        <style>
            .authors a,
            .authors a * {
                text-decoration: none !important;
                border-bottom: none !important;
                text-decoration-line: none !important;
                text-decoration-style: none !important;
                text-decoration-color: transparent !important;
            }
            .authors a:visited,
            .authors a:focus,
            .authors a:active,
            .authors a:visited *,
            .authors a:focus *,
            .authors a:active * {
                text-decoration: none !important;
                border-bottom: none !important;
                text-decoration-line: none !important;
                text-decoration-style: none !important;
                text-decoration-color: transparent !important;
            }
            .authors a:hover,
            .authors a:hover * {
                color: #007bff !important;
                text-decoration: underline !important;
                text-decoration-line: underline !important;
                text-decoration-style: solid !important;
                text-decoration-color: #007bff !important;
            }
        </style>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Test-Set Stress-Test (TsT)</i></h1>
                    <!-- <h2>‚ÄúTrain on the Test Set‚Äù ...<br> to Stress-Test Multimodal Benchmarks</h2> -->
                    <h2>Benchmark Designers Should "Train on the Test Set"<br> to Expose Exploitable Non-Visual Shortcuts
                    </h2>
                    <p class="header-lede">
                        <!-- TsT reveals how much can be solved without vision and helps designers ensure that their benchmarks truly require visual reasoning. -->
                        TsT helps multimodal benchmark designers <i>diagnose</i> and <i>mitigate</i> non-visual shortcuts in their benchmarks.
                    </p>
                    <div class="icon-container">
                        <div class="icon-item">
                            <img src="./static/img/icons/positioning.png" alt="Principle icon">
                            <div>
                                <strong>Principle.</strong>
                                If a benchmark can be gamed, it will be, so designers should proactively try to "game" their own benchmarks first.
                            </div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/eval.svg" alt="Diagnostic icon">
                            <div>
                                <strong>TsT Diagnostic.</strong>
                                Fine-tuning a powerful LLM via \(k\)-fold cross-validation on exclusively the non-visual, textual inputs of the test set to unveil shortcut performance
                            </div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/data_v2.svg" alt="Interpretability icon" class="icon">
                            <div>
                                <strong>Interpretable Audit.</strong>
                                A lightweight Random Forest version (TsT-RF) with hand-crafted features shows which lexical and structural patterns drive shortcuts.
                            </div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/nn_model.svg" alt="Mitigation icon" class="icon">
                            <div>
                                <strong>Iterative Bias Pruning.</strong>
                                Per-sample bias scores \(s(x)\) enables iterative pruning of shortcut-prone questions.
                            </div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/visual.svg" alt="Benchmarks icon" class="icon">
                            <div>
                                <strong>Real Benchmarks.</strong>
                                On VSI-Bench, CV-Bench, MMMU, and VideoMME, TsT exposes +30% blind gains and yields a debiased VSI-Bench with a much larger vision-blind gap.
                            </div>
                        </div>
                    </div>
                    <div class="button-container">
                        <a href="https://arxiv.org/abs/2511.04655" class="button">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            <span>arXiv</span>
                        </a>
                        <a href="https://github.com/vision-x-nyu/test-set-training" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        
                        <a href="https://hf.co/datasets/nyu-visionx/VSI-Bench" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                                alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VSI-Bench (Debiased)</span>
                        </a>
                        
                        <a href="https://hf.co/datasets/nyu-visionx/VSI-Train-10k" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                                alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>VSI-Train-10k</span>
                        </a>
                        
                        <a href="https://x.com/_ellisbrown/status/1988014436645327220" class="button" target="_blank">
                            <span class="icon is-small">
                                üßµ
                            </span>
                            <span>Thread</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img src="static/img/header.png" class="header-hero-image" alt="Test-set Stress-Test illustration" draggable="false" loading="eager" fetchpriority="high" decoding="async">
                </div>
            </div>
        </div>
        
        <d-article>
            <div class="byline">
                <div class="byline-container">
                    <div class="authors" style="text-align: center; margin-bottom: 1rem;">
                        <div style="margin-bottom: 0.5rem;">
                            <a href="https://ellisbrown.github.io/" target="_blank"><span class="author-link">Ellis Brown</span></a> &emsp;
                            <a href="https://jihanyang.github.io/" target="_blank"><span class="author-link">Jihan Yang</span></a> &emsp;
                            <a href="https://github.com/vealocia" target="_blank"><span class="author-link">Shusheng Yang</span></a> &emsp;
                            <a href="https://cs.nyu.edu/~fergus/" target="_blank"><span class="author-link">Rob Fergus</span></a> &emsp;
                            <a href="https://www.sainingxie.com/" target="_blank"><span class="author-link">Saining Xie</span></a>
                        </div>
                    </div>
                    <a style="text-align: center;" href="https://cs.nyu.edu/home/index.html" class="affiliation-link" id="affiliation" target="_blank">New York University</a>
                </div>
            </div>
        
            <section id="abstract">
                <h1 class="text">Abstract</h1>
                <p class="text">
                    Robust multimodal benchmarks are the foundation of progress in Multimodal Large Language Models (MLLMs).
                    Yet we show that many modern ‚Äúvision-centric‚Äù benchmarks can be <em>aced</em> by models that ignore the visual input entirely,
                    simply by exploiting non-visual shortcuts encoded in the test set‚Äôs questions, answer distributions, and templates.
                </p>
                <p class="text">
                    We advocate a simple principle for benchmark design:
                    <strong>if a benchmark can be gamed, it will be</strong>.
                    Rather than hoping blind baselines remain low, we propose that benchmark designers should actively
                    <em>train on the test set</em> to probe its intrinsic vulnerabilities.
                </p>
                <p class="text">
                    We introduce the <strong>Test-set Stress-Test (TsT)</strong>, which uses \(k\)-fold cross-validation on non-visual test-set information
                    to (<em>i</em>) estimate a benchmark‚Äôs global non-visual solvability and (<em>ii</em>) assign sample-level bias scores \(s(x)\).
                    We instantiate TsT with a powerful LLM-based diagnostic (TsT-LLM) and an interpretable Random Forest diagnostic (TsT-RF).
                    Finally, we propose <strong>Iterative Bias Pruning (IBP)</strong>, which uses TsT‚Äôs bias scores to iteratively filter high-bias samples
                    and construct more robust datasets.
                </p>
                <p class="text">
                    Applied to four widely used benchmarks&mdash;VSI-Bench, CV-Bench, MMMU, and VideoMME&mdash;TsT reveals
                    <strong>pervasive non-visual shortcuts</strong>, including gains of more than <strong>+30 percentage points</strong> in blind accuracy
                    by learning test-set patterns alone. As a case study, we use TsT + IBP to create a debiased variant of VSI-Bench that
                    substantially widens the vision‚Äìblind performance gap and better compels genuine visual reasoning.
                </p>
            </section>
        
            <h1 class="text">Overview</h1>
            <p class="text">
                Modern multimodal benchmarks have evolved from tightly controlled visual tasks to open-ended question-answering
                over images and videos. This increased expressivity comes with a hidden cost:
                <strong>it is much harder to know what is actually being measured</strong>.
                A model can score highly by exploiting world knowledge and textual regularities,
                without truly understanding the visual content.
            </p>
            <p class="text">
                We focus on <em>non-visual shortcuts</em>:
                cases where questions can be answered correctly without using the visual input at all.
                These shortcuts can come from natural world knowledge (e.g., "fridges are usually around 170‚Äì180cm tall"),
                or from statistical quirks of the benchmark (e.g., certain answers appearing disproportionately often,
                or specific templates almost always mapping to the same label).
                Either way, when the goal is to measure <em>visual</em> understanding, such patterns undermine evaluation.
            </p>
        
            <section id="teaser" style="margin-top: 2rem; margin-bottom: 4rem;">
                <d-figure>
                    <figure>
                        <img src="static/img/teaser.png" alt="TsT overview teaser" class="pdf-figure" style="width: auto; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption>
                            <strong>Figure:</strong>
                            The Evolving Landscape of Visual Understanding Benchmarks. As benchmarks evolved from controlled, narrow tasks to open-ended VQA, they gained expressivity but became vulnerable to non-visual shortcuts. Language-driven evaluation enables flexible querying but risks models exploiting linguistic patterns rather than visual understanding.
                        </figcaption>
                    </figure>
                </d-figure>
            </section>

            <section id="bias-examples" style="margin-top: 2rem; margin-bottom: 4rem;">
                <h2 class="text">Statistical Biases Create Non-Visual Shortcuts</h2>
                <p class="text">
                    To make this concrete, here are four types of statistical biases we discovered across real benchmarks.
                    These patterns enable models to achieve high accuracy without visual reasoning:
                </p>
                <d-figure>
                    <figure>
                        <img src="static/img/bias_examples.png" alt="Statistical bias examples" class="pdf-figure" style="width: 100%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption>
                            <strong>Figure:</strong>
                            <strong>(a) Counting:</strong> VSI-Bench shows severe answer skew‚Äîover 50% of questions have ground truth ‚â§3, enabling high accuracy by always guessing "2".
                            <strong>(b) Spatial Relation:</strong> In CV-Bench depth, certain categories like "keyboard" and "clothes" appear as the correct answer 100% of the time.
                            <strong>(c) Appearance Order:</strong> "Clock" appears in the 4th position in 100% of VSI-Bench questions where it appears.
                            <strong>(d) Size Estimation:</strong> Room sizes cluster around typical dimensions (log Œº ‚âà 17m¬≤), making them predictable without seeing the room.
                        </figcaption>
                    </figure>
                </d-figure>
            </section>
        
            <ul class="text">
                <li>
                    <strong>Definition of non-visual shortcuts.</strong>
                    We formalize when a learnable pattern counts as an <em>exploitable shortcut</em>:
                    whenever it allows models to bypass the visual modality in a benchmark intended to measure visual understanding.
                </li>
                <li>
                    <strong>Test-set Stress-Test (TsT).</strong>
                    We propose training a blind diagnostic model directly on the test set (via \(k\)-fold cross-validation)
                    to <em>quantify</em> how much of the benchmark can be solved from non-visual signals alone.
                </li>
                <li>
                    <strong>Sample-level bias scores.</strong>
                    TsT yields per-question scores \(s(x)\) indicating how reliably each sample can be answered without vision,
                    providing a concrete target for mitigation.
                </li>
                <li>
                    <strong>Iterative Bias Pruning.</strong>
                    Using \(s(x)\), IBP iteratively filters high-bias samples, re-diagnoses the remaining dataset,
                    and repeats until non-visual shortcuts are substantially reduced.
                </li>
                <li>
                    <strong>Case study on real benchmarks.</strong>
                    Across VSI-Bench, CV-Bench, MMMU, and VideoMME, TsT exposes large gains in blind accuracy
                    (up to +33 points), and IBP produces a debiased VSI-Bench variant that better isolates visual reasoning.
                </li>
            </ul>
        
            <section id="shortcuts">
                <h1 class="text">Non-Visual Shortcuts Undermine Multimodal Evaluation</h1>
                
                <h3 class="text" style="margin-top: 2rem;">Knowledge-Based Shortcuts</h3>
                <p class="text">
                    The first category of shortcuts comes from world knowledge embedded in LLMs during pretraining.
                    As shown below, benchmarks like MMMU benefit more from scaling the LLM backbone than from enabling vision,
                    suggesting they rely heavily on linguistic knowledge. In contrast, VSI-Bench shows negligible gains from 
                    LLM scaling in blind settings but substantial improvements when vision is enabled‚Äîdemonstrating greater 
                    robustness to knowledge-based shortcuts.
                </p>
                
                <d-figure style="margin: 2rem 0;">
                    <figure>
                        <img src="static/img/blind_ov.png" alt="Knowledge-based shortcuts" class="pdf-figure" style="width: 95%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption>
                            <strong>Figure:</strong>
                            <strong>Knowledge-based shortcuts in multimodal benchmarks.</strong>
                            Blind (red squares) vs. vision-enabled (blue circles) performance across LLaVA-OneVision model scales.
                            MMMU shows substantial gains from scaling the LLM backbone (x-axis) but minimal improvement from enabling vision (y-axis),
                            indicating reliance on linguistic knowledge. VSI-Bench demonstrates the opposite pattern‚Äîlarge vision gains with negligible blind scaling‚Äîconfirming robustness to knowledge-based shortcuts. VideoMME shows roughly equal gains from both sources, while CV-Bench benefits more from vision but still exhibits significant gains from LLM scaling.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <h3 class="text" style="margin-top: 2rem;">Statistical Shortcuts</h3>
                <p class="text">
                    Not every pattern in a dataset is a shortcut.
                    The key question is not where a pattern comes from (world statistics vs. annotation artifacts),
                    but what <em>effect</em> it has on the evaluation.
                    If a model can exploit a pattern to answer correctly without using the visual signal,
                    then for a vision-centric benchmark, that pattern is a problem.
                </p>
                <p class="text">
                    For example, consider questions like "Which item is closest to the bed?" where "lamp"
                    happens to be the correct answer far more often than chance.
                    Even if this reflects some real-world regularity, in a benchmark that is supposed to probe spatial reasoning,
                    it lets models answer correctly by leaning on text-only priors rather than the actual image.
                </p>
        
                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.05em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        <strong>Litmus test:</strong>
                        if a blind model can reach high accuracy on a ‚Äúvision‚Äù benchmark, its scores no longer reliably reflect visual understanding.
                    </p>
                </div>
        
                <p class="text">
                    We distinguish between two failure modes:
                </p>
                <ul class="text">
                    <li>
                        <strong>Training failures:</strong>
                        biased or insufficient training data prevent models from learning the intended capability,
                        even if the test set is well-designed.
                    </li>
                    <li>
                        <strong>Evaluation failures:</strong>
                        the test set itself contains patterns that allow models to score well for the wrong reasons,
                        regardless of how they were trained.
                    </li>
                </ul>
                <p class="text">
                    TsT specifically targets <em>evaluation failures</em>:
                    it asks how much of the test set can be solved by learning patterns in the test questions and answers alone.
                </p>
            </section>
        
            <section id="tst-framework">
                <h1 class="text">The Test-set Stress-Test (TsT) Framework</h1>
                <p class="text">
                    At a high level, TsT performs \(k\)-fold cross-validation directly on the benchmark‚Äôs test set,
                    using <em>only non-visual information</em> (text, metadata, templates).
                    For each fold, we train a blind diagnostic model on the remaining folds and evaluate it on the held-out fold.
                    Every test example is thus predicted by a model that has not seen that example during training,
                    but <em>has</em> seen the rest of the test set.
                </p>
        
                <d-figure>
                    <figure>
                        <div style="display: flex; gap: 1rem; align-items: center; justify-content: center; flex-wrap: wrap;">
                            <img src="static/img/TsT_overview_bias_space.png" alt="Bias Space" class="pdf-figure" style="width: 30%; height: auto; border: none;" data-zoomable="" draggable="false">
                            <img src="static/img/TsT_overview.png" alt="TsT framework overview" class="pdf-figure" style="width: 52%; height: auto; border: none;" data-zoomable="" draggable="false">
                        </div>
                        <figcaption>
                            <strong>Figure:</strong>
                            <strong>(Left)</strong> TsT directly probes biases intrinsic to the specific test set (pink region), rather than approximating them via external training data.
                            <strong>(Right)</strong> The test set is partitioned into \(k\) folds, a blind diagnostic model is trained on \(k{-}1\) folds and evaluated on the held-out fold,
                            and this is repeated until all samples are covered.
                            Aggregating across folds yields both a global non-visual solvability estimate and per-sample bias scores \(s(x)\).
                        </figcaption>
                    </figure>
                </d-figure>
        
                <p class="text">
                    TsT produces two key outputs:
                </p>
                <ul class="text">
                    <li>
                        <strong>TsT accuracy.</strong>
                        The overall accuracy of the blind diagnostic across all folds.
                        High TsT accuracy means a large fraction of the benchmark is solveable from non-visual signals alone,
                        implying strong exploitable shortcuts.
                    </li>
                    <li>
                        <strong>Sample-level bias score \(s(x)\).</strong>
                        For each sample, \(s(x)\) is the empirical probability that TsT predicts the correct answer
                        when that sample is in the validation fold.
                        High \(s(x)\) indicates samples that are consistently answerable without vision and are prime candidates for pruning or rewriting.
                    </li>
                </ul>
            </section>
        
            <section id="tst-variants">
                <h1 class="text">TsT-LLM and TsT-RF: Two Complementary Diagnostics</h1>
                <p class="text">
                    We instantiate TsT with two complementary diagnostics here.
                </p>
                <h3 class="text">TsT-LLM: Power from the Same Model Class</h3>
                <p class="text">
                    <strong>TsT-LLM</strong> uses a strong language model (e.g., Qwen2.5-7B) as the diagnostic.
                    For each fold, we LoRA-tune the LLM on question-only inputs from the training folds
                    and evaluate on held-out questions.
                    This requires no hand-designed features and can capture both simple statistical patterns and complex knowledge-based shortcuts.
                </p>
                <p class="text">
                    On template-based benchmarks like CV-Bench and VSI-Bench, TsT-LLM dramatically increases blind accuracy:
                    from 40.1&nbsp;&rarr;&nbsp;73.4 on CV-Bench and 25.0&nbsp;&rarr;&nbsp;56.4 on VSI-Bench,
                    revealing <strong>+33.3</strong> and <strong>+31.4</strong> point gains purely from learning test-set text.
                    Even on more heterogeneous benchmarks like MMMU and VideoMME, TsT-LLM finds sizeable gains of +8.6 and +6.4 points.
                </p>
        
                <h3 class="text">TsT-RF: Fast and Interpretable</h3>
                <p class="text">
                    <strong>TsT-RF</strong> uses a Random Forest classifier trained on lightweight, human-interpretable features
                    (e.g., answer frequencies, template IDs, question length, lexical indicators).
                    While less expressive than TsT-LLM, it is CPU-friendly and provides direct insight into <em>which</em> patterns
                    the diagnostic is exploiting, via feature importances.
                </p>
                <p class="text">
                    Together, TsT-LLM and TsT-RF deliver both strong detection of shortcut behavior and actionable explanations
                    of how benchmark structure contributes to non-visual solvability.
                </p>
            </section>
        
            <section id="ibp">
                <h1 class="text">Iterative Bias Pruning (IBP)</h1>
                <p class="text">
                    TsT does more than say ‚Äúyour benchmark has shortcuts‚Äù.
                    Its sample-level bias scores \(s(x)\) provide a <em>ranking</em> of which questions are most vulnerable.
                    <strong>Iterative Bias Pruning (IBP)</strong> turns this into a systematic procedure for improving a benchmark.
                </p>
        
                <d-figure>
                    <figure>
                        <!-- TODO: replace with IBP algorithm / visualization figure -->
                        <img src="static/img/IBP.png" alt="Iterative Bias Pruning overview" class="pdf-figure" style="width: 100%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption>
                            <strong>Figure:</strong>
                            IBP loop.
                            At each iteration, TsT is re-run to compute \(s(x)\) on the current dataset,
                            a batch of the most shortcut-prone samples is removed or revised,
                            and the process repeats until non-visual solvability drops below a threshold or a removal budget is reached.
                        </figcaption>
                    </figure>
                </d-figure>
        
                <p class="text">
                    Concretely, IBP:
                </p>
                <ul class="text">
                    <li>Runs TsT to compute \(s(x)\) for all samples in the dataset.</li>
                    <li>Removes (or rewrites) a batch of the highest-\(s(x)\) samples.</li>
                    <li>Repeats diagnosis on the remaining dataset, stopping early when all \(s(x)\) fall below a tolerance.</li>
                </ul>
                <p class="text">
                    IBP is agnostic to the specific diagnostic (TsT-LLM or TsT-RF) and to the mitigation action
                    (pruning, rewriting, rebalancing). In this work, we focus on pruning as a proof-of-concept.
                </p>
            </section>
        
            <section id="case-study">
                <h1 class="text">Case Study: Debiasing VSI-Bench</h1>
                <p class="text">
                    As a concrete demonstration, we apply TsT + IBP to <strong>VSI-Bench</strong>, a spatial reasoning benchmark.
                    TsT-LLM shows that a blind model can gain over 30 points of accuracy by training on test-set questions alone,
                    indicating strong non-visual shortcuts.
                </p>
                <p class="text">
                    IBP uses TsT-RF bias scores to prune shortcut-prone questions and produces a <strong>VSI-Bench-Debiased</strong> variant.
                    We then re-evaluate LLaVA-Video-7B before and after fine-tuning on additional in-distribution data:
                </p>
                
                <div id="tab:vsi-bench-debiased" style="display: flex; flex-direction: column; align-items: center;">
                <div class="table-container" style="margin: 1.5rem auto;">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th class="tb-hdr"></th>
                                <th class="tb-hdr">Vis.</th>
                                <th class="tb-hdr">Blind</th>
                                <th class="tb-hdr">\(\Delta_{V-B}\)</sub></th>
                                <th class="tb-hdr">Vis. (Debiased)</th>
                                <th class="tb-hdr">Blind (Debiased)</th>
                                <th class="tb-hdr">\(\Delta_{V-B}\)</sub> (Debiased)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>LLaVA-Video 7B (base)</td>
                                <td>36.7</td>
                                <td>25.9</td>
                                <td><em>10.8</em></td>
                                <td>31.3</td>
                                <td>20.3</td>
                                <td><em>11.0</em></td>
                            </tr>
                            <tr>
                                <td>+ VSI-Train-10k FT</td>
                                <td>57.1</td>
                                <td>44.7</td>
                                <td><em>12.4</em></td>
                                <td>48.7</td>
                                <td>32.0</td>
                                <td><em>16.6</em></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption style="text-align: center; margin-top: 0.5rem;">
                    <strong>Table:</strong>
                    On the original VSI-Bench, fine-tuning boosts both vision and blind scores,
                    masking how much progress is due to text-only shortcuts.
                    On VSI-Bench-Debiased, the blind score falls more sharply,
                    creating a significantly larger vision‚Äìblind gap and better reflecting true visual gains.
                </figcaption>
                </div>
        
                <p class="text">
                    This case study illustrates TsT‚Äôs full lifecycle:
                    diagnose non-visual shortcuts, compute sample-level bias scores, prune the worst offenders,
                    and re-evaluate to confirm that visual reasoning, not text-only priors, drives progress.
                </p>
            </section>
        
            <section id="diagnostics-summary">
                <h1 class="text">Diagnostics Across Four Benchmarks</h1>
                <p class="text">
                    Beyond VSI-Bench, TsT reveals pervasive shortcuts across three additional benchmarks:
                    <strong>CV-Bench</strong>, <strong>MMMU</strong>, and <strong>VideoMME</strong>.
                    In each case, TsT-LLM significantly improves blind accuracy simply by training on the test questions and answers.
                </p>
        
                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.05em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        <strong>TsT-LLM results:</strong>
                        blind accuracy climbs from 40.1&nbsp;&rarr;&nbsp;73.4 on CV-Bench and 25.0&nbsp;&rarr;&nbsp;56.4 on VSI-Bench,
                        with additional gains of +8.6 on MMMU and +6.4 on VideoMME ‚Äî
                        all <em>without</em> using any visual input.
                    </p>
                </div>
        
                <p class="text">
                    These findings highlight that shortcut behavior is not an isolated issue in any single dataset,
                    but a structural risk across diverse benchmark designs, including template-based, human-authored,
                    and LLM-generated questions.
                </p>
            </section>
        
            <section id="takeaways">
                <h1 class="text">Takeaways for Benchmark Designers</h1>
                <p class="text">
                    TsT is meant to be a <em>practical</em> tool for anyone designing or maintaining multimodal benchmarks.
                    From our analysis, we propose a set of actionable guidelines:
                </p>
                <ul class="text">
                    <li>
                        <strong>Always include a ‚Äútrained blind‚Äù baseline.</strong>
                        Zero-shot blind accuracy is not enough. Train a blind model on the test set via TsT and report its performance.
                    </li>
                    <li>
                        <strong>Track both global and per-sample bias.</strong>
                        Use TsT accuracy to quantify overall non-visual solvability, and \(s(x)\) to understand which samples are most affected.
                    </li>
                    <li>
                        <strong>Iteratively refine the test set.</strong>
                        Use IBP (or similar) to prune or rewrite the most shortcut-prone questions until TsT accuracy falls to an acceptable range.
                    </li>
                    <li>
                        <strong>Distinguish training vs. evaluation failures.</strong>
                        High TsT accuracy indicates that evaluation is compromised, even if models are trained ‚Äúcorrectly.‚Äù
                    </li>
                    <li>
                        <strong>Integrate TsT into the benchmark lifecycle.</strong>
                        Treat stress-testing your test set as a standard step, not an afterthought.
                    </li>
                </ul>
        
                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.05em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        Benchmark designers <strong>should</strong> ‚Äútrain on the test set‚Äù ‚Äî
                        not to inflate scores, but to adversarially audit evaluation instruments
                        and ensure that reported progress reflects genuine multimodal understanding.
                    </p>
                </div>
            </section>
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{brown2025shortcuts,<br>
                    &nbsp;&nbsp;author = {Brown, Ellis and Yang, Jihan and Yang, Shusheng and Fergus, Rob and Xie, Saining},<br>
                    &nbsp;&nbsp;title = {Benchmark Designers Should ``Train on the Test Set'' to Expose Exploitable Non-Visual Shortcuts},<br>
                    &nbsp;&nbsp;journal = {arXiv preprint arXiv:2511.04655},<br>
                    &nbsp;&nbsp;year = {2025}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
    </body>
</html>